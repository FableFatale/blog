


[{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/tags/cloudflare/","section":"Tags","summary":"","title":"Cloudflare","type":"tags"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/tags/css/","section":"Tags","summary":"","title":"CSS","type":"tags"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/","section":"Rafael Yu's Blog","summary":"","title":"Rafael Yu's Blog","type":"page"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"26 November 2025","externalUrl":null,"permalink":"/categories/tech/","section":"Categories","summary":"","title":"Tech","type":"categories"},{"content":"","date":"26 November 2025","externalUrl":null,"permalink":"/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting","type":"tags"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/","section":"Tags","summary":"","title":"故障排除","type":"tags"},{"content":"","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/categories/%E6%8A%80%E6%9C%AF/","section":"Categories","summary":"","title":"技术","type":"categories"},{"content":"当将 Hugo 网站（特别是使用 Blowfish 主题时）部署到 Cloudflare Pages 时，你可能会遇到网站加载后没有任何样式（CSS 丢失）的问题。浏览器控制台通常会显示与 子资源完整性 (SRI) 校验失败相关的错误。\n问题现象\r#\r网站显示“乱码”或没有样式。检查浏览器开发者控制台 (F12) 会发现类似以下的错误：\nFailed to find a valid digest in the \u0026lsquo;integrity\u0026rsquo; attribute for resource \u0026lsquo;\u0026hellip;\u0026rsquo; with computed SHA-512 integrity \u0026lsquo;\u0026hellip;\u0026rsquo;. The resource has been blocked. (无法在资源 \u0026lsquo;\u0026hellip;\u0026rsquo; 的 \u0026lsquo;integrity\u0026rsquo; 属性中找到有效的摘要，计算出的 SHA-512 完整性为 \u0026lsquo;\u0026hellip;\u0026rsquo;。资源已被拦截。)\n问题原因\r#\r这是由于 Cloudflare 的自动压缩 (Auto Minify) 功能导致的。\nHugo 生成 CSS/JS 文件并计算其加密哈希值 (SRI)，将其添加到 HTML 的 integrity 属性中。 Cloudflare Pages（如果启用了自动压缩）会修改这些文件以减小体积。 浏览器 下载修改后的文件，计算其哈希值，并与 HTML 中的哈希值进行比较。由于文件内容变了，哈希值不匹配，浏览器出于安全原因拦截了该资源。 解决方案\r#\r由于 Cloudflare 已于 2024 年 8 月弃用了 Auto Minify (自动压缩) 功能，之前建议禁用该功能的方案已不再适用。\n现在的主要解决方案是移除 Hugo 模板中的 integrity 属性。这可以防止浏览器因 Cloudflare 边缘优化或压缩导致的哈希不匹配而拦截资源。\n移除 Integrity 属性\r#\r你需要覆盖主题的模板，移除 CSS 和 JS 标签中的 integrity 属性。\n步骤：\n复制模板文件： 将主题的 head.html 局部模板复制到你的项目布局文件夹中： themes/blowfish/layouts/partials/head.html -\u0026gt; layouts/partials/head.html\n编辑文件： 打开新复制的 layouts/partials/head.html 文件。\n移除 Integrity 属性： 找到所有包含 integrity 属性的 \u0026lt;link\u0026gt; 和 \u0026lt;script\u0026gt; 标签，并删除 integrity=\u0026quot;{{ ... }}\u0026quot; 部分。\n示例 (CSS):\n将：\n\u0026lt;link type=\u0026#34;text/css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ $bundleCSS.RelPermalink }}\u0026#34; integrity=\u0026#34;{{ $bundleCSS.Data.Integrity }}\u0026#34;\u0026gt; 改为：\n\u0026lt;link type=\u0026#34;text/css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ $bundleCSS.RelPermalink }}\u0026#34;\u0026gt; 示例 (JS):\n将：\n\u0026lt;script src=\u0026#34;{{ $js.RelPermalink }}\u0026#34; integrity=\u0026#34;{{ $js.Data.Integrity }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 改为：\n\u0026lt;script src=\u0026#34;{{ $js.RelPermalink }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 重新构建： 重新构建你的网站。资源将不再进行完整性校验，从而彻底解决样式丢失的问题。\n","date":"2025年11月26日","externalUrl":null,"permalink":"/zh-cn/posts/%E8%A7%A3%E5%86%B3-cloudflare-pages-%E4%B8%8A-css-%E6%A0%B7%E5%BC%8F%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98integrity-%E5%B1%9E%E6%80%A7/","section":"Posts","summary":"\u003cp\u003e当将 Hugo 网站（特别是使用 Blowfish 主题时）部署到 Cloudflare Pages 时，你可能会遇到网站加载后没有任何样式（CSS 丢失）的问题。浏览器控制台通常会显示与 \u003cstrong\u003e子资源完整性 (SRI)\u003c/strong\u003e 校验失败相关的错误。\u003c/p\u003e","title":"解决 Cloudflare Pages 上 CSS 样式丢失问题（Integrity 属性）","type":"posts"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/cs/","section":"Tags","summary":"","title":"Cs","type":"tags"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/fewshot/","section":"Tags","summary":"","title":"Fewshot","type":"tags"},{"content":"Essentially it’s a 2 step method (although you can use a single prompt in some settings):\nStep 1: Break down complex problems into simpler subproblems Step 2: Solve the problems sequentially Here’s an example of least-to-most prompting via a single prompt:\nQ: It takes John 3 minutes to build a tower with blocks. It takes him 2 minutes to knock it down. The playtime ends in 20 minutes. How many times can he build and knock down the tower before playtime ends? A: To solve the problem “How many times can John build and knock down the tower before playtime ends?”, we need to:\nDetermine the total time it takes for one complete cycle (build + knock down). Calculate how many complete cycles he can do within the available time of 20 minutes. Q: It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down. The water slide closes in 15 minutes. How many times can she slide before it closes? A: I like this method more than chain-of-thought because ti explicitly breaks the problem down into more manageable steps. This makes it easier to use this method for any task.\nAdditionally, in the head-to-head experiments it was able to consistently outperform chain-of-thought across a variety of tasks.\nI put together three prompts that you can use to run least-to-most prompting for any problem.\nPrompt 1: A prompt that will generate few-shot examples showing the model how to break down problems\nYour job is to generate few-shot examples for the following task: {{ task }}\nYour few-shot examples should contain two parts: A problem, and the decomposed subproblems. It should follow the structure below:\n\u0026quot;\u0026quot;\u0026quot;\nProblem: Problem description\nDecomposed subproblems:\nSubproblem 1 Subproblem 2 Subproblem 3 \u0026quot;\u0026quot;\u0026quot;\nYour output should contain only the examples, no preamble\nPrompt 2: Break down the task at hand into subproblems (with the previous output used as few-shot examples)\n{{ task }}\nList only the decomposed subproblems that must be solved before solving the task listed above. Your output should contain only the decomposed subproblems, no preamble\nHere are a few examples of problems and their respective decomposed subproblems: {{ few-shot-examples}}\nPrompt 3: Pass the subproblems and solve the task!\nSolve the following task by addressing the subproblems listed below.\nTask: {{ task }}\nSubproblems: {{sub-problems}}\n","date":"22 August 2024","externalUrl":null,"permalink":"/posts/least-to-most-prompting/","section":"Posts","summary":"\u003cp\u003eEssentially it’s a 2 step method (although you can use a single prompt in some settings):\u003c/p\u003e","title":"LEAST-TO-MOST PROMPTING","type":"posts"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/prompt/","section":"Tags","summary":"","title":"Prompt","type":"tags"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/skills/","section":"Tags","summary":"","title":"Skills","type":"tags"},{"content":"\rThe Transformer Model: Revolutionizing Natural Language Processing\r#\rKeywords: transformer model, natural language processing, NLP, machine learning, artificial intelligence, deep learning\nIn the world of artificial intelligence and natural language processing, few innovations have made as significant an impact as the transformer model. This groundbreaking architecture has revolutionized how machines understand and generate human language. Let’s dive into the key components that make transformers so powerful.\nThe Building Blocks of a Transformer\r#\rThe transformer architecture consists of several crucial elements:\nTokenization Embedding Positional encoding Transformer blocks (multiple) Softmax layer Each of these components plays a vital role in the model’s ability to process and generate language. Let’s explore them in detail.\n1. Tokenization: Breaking Language into Manageable Pieces\r#\rTokenization is the first step in processing text for a transformer model. It involves breaking down text into smaller units called tokens. These tokens can be words, subwords, or even individual characters, depending on the specific tokenization method used.\nDifferent tokenization strategies can significantly impact a model’s performance and efficiency. For instance, subword tokenization allows models to handle rare words and different languages more effectively.\n2. Embedding: Turning Words into Numbers\r#\rOnce text is tokenized, the next step is to convert these tokens into a format that machines can understand – numbers. This is where embedding comes in. Embedding turns each token into a vector of numbers, representing its meaning in a multi-dimensional space.\nThese embeddings capture semantic relationships between words, allowing the model to understand context and meaning. Different transformer models may use embeddings of varying dimensions, typically ranging from 128 to 1024 or more.\n3. Positional Encoding: Preserving Word Order\r#\rUnlike recurrent neural networks, transformer models process all words in a sentence simultaneously. This parallel processing is efficient but loses information about word order. Positional encoding solves this problem by adding information about a token’s position in the sequence to its embedding.\nThis clever technique ensures that the model can distinguish between sentences with the same words in different orders, preserving crucial syntactic information.\n4. Transformer Blocks: The Heart of the Model\r#\rThe transformer block is where the magic happens. Each block consists of two main components:\nThe attention mechanism The feedforward neural network Multiple transformer blocks are typically stacked on top of each other, allowing the model to learn increasingly complex language patterns.\nAttention: Understanding Context\r#\rThe attention mechanism is a key innovation in transformer models. It allows the model to focus on different parts of the input when processing each word, capturing context and relationships between words.\nFor example, in the sentences “The bank of the river” and “Money in the bank,” the attention mechanism helps the model understand the different meanings of “bank” based on context.\nTransformers often use multi-head attention, where several different attention mechanisms operate in parallel, allowing the model to capture various types of relationships simultaneously.\n5. The Softmax Layer: Generating Probabilities\r#\rThe final layer of a transformer model is typically a softmax layer. This layer takes the raw outputs from the transformer blocks and converts them into probabilities.\nFor tasks like language generation, these probabilities represent the likelihood of each word in the vocabulary being the next word in the sequence. The word with the highest probability is often chosen as the model’s output.\nBeyond Training: Fine-Tuning and Adaptation\r#\rWhile the basic architecture of transformer models is powerful, their true potential is realized through fine-tuning and adaptation. Post-training techniques allow these models to excel in specific tasks or domains.\nFor instance, a general-purpose transformer can be fine-tuned on conversational data to create a chatbot, or on code repositories to assist with programming tasks. This flexibility and adaptability have made transformers the backbone of many modern AI applications.\nConclusion: The Future of Language AI\r#\rThe transformer architecture has fundamentally changed the landscape of natural language processing. From machine translation to chatbots, from content generation to code completion, transformers are powering a new generation of AI capabilities.\nAs research continues and these models become more sophisticated, we can expect even more impressive applications in the future. The era of truly intelligent language AI is just beginning, and transformers are leading the way.\nThank you, Cohere. And thank you for your reading.\r#\r","date":"18 August 2024","externalUrl":null,"permalink":"/posts/the-architecture-of-a-transformer-model/","section":"Posts","summary":"\u003ch1 class=\"relative group\"\u003eThe Transformer Model: Revolutionizing Natural Language Processing\r\n    \u003cdiv id=\"the-transformer-model-revolutionizing-natural-language-processing\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#the-transformer-model-revolutionizing-natural-language-processing\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h1\u003e\r\n\u003cp\u003e\u003cstrong\u003eKeywords\u003c/strong\u003e: transformer model, natural language processing, NLP, machine learning, artificial intelligence, deep learning\u003c/p\u003e","title":"The architecture of a transformer model","type":"posts"},{"content":"","date":"18 August 2024","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":"Analyze the following prompt idea: [insert prompt idea]~Rewrite the prompt for clarity and effectiveness~Identify potential improvements or additions~Refine the prompt based on identified improvements~Present the final optimized prompt Example:\nInput:\nAnalyze the following prompt idea: [You are an experienced financial analyst and investment advisor. Please develop a long-term investment strategy for a 30-year-old professional. This individual has an annual income of $100,000, with $50,000 available for immediate investment, and plans to invest an additional $1,000 monthly. They have a moderate risk tolerance and aim to prepare for retirement in 25-30 years. Provide specific asset allocation recommendations, including stocks, bonds, ETFs, and other potential investment vehicles. Explain the rationale behind your recommendations and discuss the potential risks and returns of this strategy.]Rewrite the prompt for clarity and effectivenessIdentify potential improvements or additionsRefine the prompt based on identified improvementsPresent the final optimized prompt\nOutput:\nStep 1: Rewrite the Prompt for Clarity and Effectiveness\r#\rOriginal Prompt:[You are an experienced financial analyst and investment advisor. Please develop a long-term investment strategy for a 30-year-old professional. This individual has an annual income of $100,000, with $50,000 available for immediate investment, and plans to invest an additional $1,000 monthly. They have a moderate risk tolerance and aim to prepare for retirement in 25-30 years. Provide specific asset allocation recommendations, including stocks, bonds, ETFs, and other potential investment vehicles. Explain the rationale behind your recommendations and discuss the potential risks and returns of this strategy.]\n**Rewritten Prompt:**You are an experienced financial analyst and investment advisor. Develop a comprehensive long-term investment strategy for a 30-year-old professional with the following profile:\nAnnual income: $100,000 Immediate investment capital: $50,000 Monthly investment: $1,000 Risk tolerance: Moderate Investment horizon: 25-30 years (targeting retirement) Provide specific recommendations for asset allocation, including stocks, bonds, ETFs, and other relevant investment vehicles. Explain the rationale behind each recommendation and discuss the potential risks and expected returns associated with the proposed strategy.\nStep 2: Identify Potential Improvements or Additions\r#\r**Incorporate tax considerations:**Address how tax-advantaged accounts (e.g., IRAs, 401(k)s) could be utilized. **Diversification within asset classes:**Suggest diversification strategies within each asset class. **Rebalancing strategy:**Include advice on how and when to rebalance the portfolio. **Inflation protection:**Discuss how to protect the portfolio against inflation. **Contingency planning:**Consider unexpected financial needs or market downturns. Step 3: Refine the Prompt Based on Identified Improvements\r#\r**Refined Prompt:**You are an experienced financial analyst and investment advisor. Develop a comprehensive long-term investment strategy for a 30-year-old professional with the following profile:\nAnnual income: $100,000 Immediate investment capital: $50,000 Monthly investment: $1,000 Risk tolerance: Moderate Investment horizon: 25-30 years (targeting retirement) Your strategy should include:\n**Asset Allocation:**Provide specific recommendations for asset allocation, including stocks, bonds, ETFs, and other relevant investment vehicles. **Tax Considerations:**Address how to leverage tax-advantaged accounts (e.g., IRAs, 401(k)s). **Diversification:**Suggest diversification strategies within each asset class. **Rebalancing:**Offer advice on how and when to rebalance the portfolio. **Inflation Protection:**Discuss strategies to protect the portfolio against inflation. **Contingency Planning:**Consider plans for unexpected financial needs or market downturns. Explain the rationale behind each recommendation and discuss the potential risks and expected returns associated with the proposed strategy.\nStep 4: Present the Final Optimized Prompt\r#\r**Final Optimized Prompt:**You are an experienced financial analyst and investment advisor. Develop a comprehensive long-term investment strategy for a 30-year-old professional with the following profile:\nAnnual income: $100,000 Immediate investment capital: $50,000 Monthly investment: $1,000 Risk tolerance: Moderate Investment horizon: 25-30 years (targeting retirement) Your strategy should include:\n**Asset Allocation:**Provide specific recommendations for asset allocation, including stocks, bonds, ETFs, and other relevant investment vehicles. **Tax Considerations:**Address how to leverage tax-advantaged accounts (e.g., IRAs, 401(k)s). **Diversification:**Suggest diversification strategies within each asset class. **Rebalancing:**Offer advice on how and when to rebalance the portfolio. **Inflation Protection:**Discuss strategies to protect the portfolio against inflation. **Contingency Planning:**Develop plans for unexpected financial needs or market downturns. Explain the rationale behind each recommendation and discuss the potential risks and expected returns associated with the proposed strategy.\n","date":"6 August 2024","externalUrl":null,"permalink":"/posts/use-prompt-to-enhance-your-propmt/","section":"Posts","summary":"\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eAnalyze the following prompt idea: [insert prompt idea]~Rewrite the prompt for clarity and effectiveness~Identify potential improvements or additions~Refine the prompt based on identified improvements~Present the final optimized prompt\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eExample:\u003c/p\u003e","title":"Use prompt to enhance your propmt","type":"posts"},{"content":"","date":"2024年8月2日","externalUrl":null,"permalink":"/zh-cn/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"2024年8月2日","externalUrl":null,"permalink":"/zh-cn/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"今天刚看完何同学最新一期的视频，感触非常大。我最早知道何同学也是从影视飓风那里知道的。何同学的最新一期视频主要讲他自己做视频过程中发生的一些故事，包括爱情、青春、努力、贵人、速度和时代浪潮。\n一开始，视频没什么观看量，但后来他调整了方向，集中资源，等到做大做强后开始做新品手机评测。后来，遇到贵人，加上自己的坚持，他如同周公解梦一样赶上了5G时代浪潮。当然，最让他火的因素还是坚持。\n无论是视频创作爆火的年代，还是现在的LLM，一些创始人身上都体现出了一种类似锤子科技的理想主义。理想主义驱动人们奋斗，实现目标。回看我自己，我也有理想主义，但没有付诸太多行动。自嘲一下：我学会了与过去失去的友情和爱情和解，但唯一没有和解的是现在这个糟糕的自己。\n没关系，LLM来了。我已经调整了我的方向和学习方法。STEM教育加费曼学习法加AI更适合我，行动加反思能让我获得进步。我正在找一个好的创业公司，跟随一个好的老板，加入他们，一起做出好的产品，方便他人，也方便自己。\n出海风浪很大，只做长期主义的事情。行动吧，渺小的Prompt Engineer。\n","date":"2024年8月2日","externalUrl":null,"permalink":"/zh-cn/posts/%E4%BB%8E%E4%BD%95%E5%90%8C%E5%AD%A6%E7%9C%8Bllm%E7%9C%8B%E7%90%86%E6%83%B3%E4%B8%BB%E4%B9%89/","section":"Posts","summary":"\u003cp\u003e今天刚看完何同学最新一期的视频，感触非常大。我最早知道何同学也是从影视飓风那里知道的。何同学的最新一期视频主要讲他自己做视频过程中发生的一些故事，包括爱情、青春、努力、贵人、速度和时代浪潮。\u003c/p\u003e","title":"从何同学看LLM看理想主义","type":"posts"},{"content":"","date":"2024年8月2日","externalUrl":null,"permalink":"/zh-cn/tags/%E6%84%9F%E8%B0%A2/","section":"Tags","summary":"","title":"感谢","type":"tags"},{"content":"","date":"2024年8月2日","externalUrl":null,"permalink":"/zh-cn/tags/%E7%90%86%E6%83%B3%E4%B8%BB%E4%B9%89/","section":"Tags","summary":"","title":"理想主义","type":"tags"},{"content":"\rChain-of-Thought (CoT)\r#\rCoT Prompting\r#\rIn my view,cot prompting adds some thinking steps ,similar to the process of putting an elephant into a refrigerator.As we all know, putting an elephant into a refrigerator requires three steps:\nOpen the refrigerator. Put the elephant in. Close the refrigerator. Above three steps is a thinking process, similarry , we can use COT prompting just add thinkg step in LLM\nFor instance ,add Let’s think step by step into llm.\nThe three steps mentioned above represent a thinking process. Similarly, we can use Chain of Thought (CoT) prompting to add thinking steps in large language models (LLMs)\n","date":"14 July 2024","externalUrl":null,"permalink":"/posts/chain-of-thought-prompt/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eChain-of-Thought (CoT)\r\n    \u003cdiv id=\"chain-of-thought-cot\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#chain-of-thought-cot\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h2\u003e\r\n\r\n\u003ch3 class=\"relative group\"\u003eCoT Prompting\r\n    \u003cdiv id=\"cot-prompting\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#cot-prompting\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003eIn my view,cot prompting adds some thinking steps ,similar to the process of putting an elephant into a refrigerator.As we all know, putting an elephant into a refrigerator requires three steps:\u003c/p\u003e","title":"Chain-of-Thought Prompt","type":"posts"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/tags/cot/","section":"Tags","summary":"","title":"Cot","type":"tags"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/tags/gpt/","section":"Tags","summary":"","title":"Gpt","type":"tags"},{"content":"\rThis is an introduction to the LLM parameters.\r#\rInference-Parameters\r#\rTemperature(Default 1 RANGE 0-2)\nhigh temperature will enhance llm’s personal, less temperature will keep llm’s level\nfor instance: while temperature =1.0, llm’s behavior will more brave , 0.1 will be stable\nA higher temperature setting can enhance the LLM’s creativity, while a lower temperature will maintain its consistency.\nFor example: When the temperature is set to 1.0, the LLM’s responses tend to be more adventurous; at 0.1, they are more conservative.\nMax Generation Length(Default: Model-dependent,Range 1 to model’s maximum)\nSet Max Generation Length can control llm’s input context’s length\nSetting the Maximum Generation Length can control the length of the input context for the LLM.\u0026quot;\nTop-K(Default 0 Range 1 to vocabulary,Common Range 40,50)\nSampling is restricted to the top k highest-probability words. For instance, with k set to 50, the selection is made from among the 50 most likely words.\nTop-p(Default 1 Range0-1 ,Common 0.9,0.95)\nSampling is conducted from the pool of words until their cumulative probability reaches p. For example, with p set to 0.9, the selection is made from among the words whose combined probabilities account for 90% of the total.\nRepetion Penalty(Default 1 Range 1.0- 2.0)\nTo mitigate the likelihood of generating repetitive words, a higher repetition penalty Range can be applied, which effectively reduces the occurrence of redundant content in the text.\nBeam Searth(Default 1,Range 1 to 10)\nBeam Search is a heuristic search algorithm that maintains multiple candidate sequences during generation, ultimately selecting the most optimal text output. Key parameters for this algorithm include the Beam Width, which determines the number of candidates retained at each step.\nLength Penalty\nLength Penalty is a setting used in text generation to help the model create text that’s not too short or too long. It works by adjusting the score of a generated text based on its length. Example: Imagine you’re asking a computer to write a summary of a book. Without Length Penalty, the summary might be too short and miss important points, or it might be too long and include unnecessary details. By using Length Penalty, you can tell the computer to aim for a summary that’s just the right length, so it includes all the key information without extra fluff.\nfrequency_penalty(Default 0 Range -2.0- +2.0)\nDescription: Penalizes words based on their frequency in the generated text, reducing repetition. Example: When set to 1.0, common words are used less often, resulting in more diverse text.\npresence_penalty(Default 0 Range -2.0- +2.0)\nDescription: Penalizes words that have already appeared in the text, encouraging the use of new words. Example: When set to 1.5, the model tends to use words that haven’t appeared yet, increasing text diversity.\nstop(Range:String or String list)\nDescription: Specifies one or more tokens at which to stop generation. Example: When set to [\u0026quot;.\u0026quot;, “!”, “?”], the model will stop generating after producing these punctuation marks.\nn (Default 1)\nDescription: Specifies how many completions to generate. Example: When set to 3, the model will generate 3 different responses.\nbest_of\nDescription: Generates multiple candidate results and returns the best n. Example: With n=2 and best_of=5, the model generates 5 candidate results and returns the 2 best ones.\nlogprobs\nDescription: Returns the most likely tokens and their log probabilities. Example: When set to 3, each generated token will be accompanied by the 3 most likely alternative options and their probabilities.\nno_repeat_ngram_size\nDescription: Prevents repetition of specified length word groups. Example: When set to 3, the model avoids generating any consecutive repetition of three-word combinations.\n[!CAUTION]\nThese parameters can be used together, but with some limitations:\nSome parameters work against each other, like high temperature with strict top-k/top-p. Beam search often overrides other sampling methods. Using many complex parameters at once can slow down generation. Not all models support every parameter. Some parameters have specific Range ranges (e.g., temperature is usually 0-2). Different tasks may need different parameter combinations. Stop conditions usually take priority over other parameters. Best practice:\nStart with defaults Adjust one parameter at a time Test thoroughly Keep notes on what works best参数名称 Parameter Name默认值 Default Value取值范围 Range描述 DescriptionTemperature 温度10-2控制输出的随机性和创造性。较高值增加创造性，较低值增加一致性。 Controls randomness and creativity of output. Higher values increase creativity, lower values increase consistency.Max Generation Length 最大生成长度模型相关 Model-dependent1 到模型最大值 1 to model’s maximum控制LLM输入上下文的长度。 Controls the length of the input context for the LLM.Top-K 前K个01 到词汇表大小 1 to vocabulary size限制采样到概率最高的K个词。常用范围40-50。 Restricts sampling to top K highest-probability words. Common range 40-50.Top-p 前p个10-1从累积概率达到p的词池中采样。常用值0.9, 0.95。 Samples from words until cumulative probability reaches p. Common values 0.9, 0.95.Repetition Penalty 重复惩罚11.0-2.0降低重复词的生成概率，减少冗余内容。 Reduces likelihood of generating repetitive words, decreasing redundant content.Beam Search 束搜索11-10保持多个候选序列，选择最优输出。关键参数包括Beam Width。 Maintains multiple candidate sequences, selects optimal output. Key parameter includes Beam Width.Length Penalty 长度惩罚1.0通常0.0到2.0 Usually 0.0 to 2.0根据长度调整生成文本的分数，控制输出长度。 Adjusts the score of generated text based on its length, controlling output length.frequency_penalty 频率惩罚0-2.0 到 2.0 -2.0 to 2.0根据词频对令牌进行惩罚。 Penalizes tokens based on their frequency.presence_penalty 存在惩罚0-2.0 到 2.0 -2.0 to 2.0根据是否出现过对令牌进行惩罚。 Penalizes tokens based on their presence.stop 停止无 None字符串或字符串列表 String or String list指定停止生成的标记。 Specifies tokens at which to stop generation.n 数量1正整数 Positive integer生成多少个完成结果。 Number of completions to generate.best_of 最佳数量1正整数，大于等于n Positive integer, ≥ n生成多个候选结果并返回最佳的n个。 Generate multiple candidates and return the best n.logprobs 对数概率null非负整数（通常0-5） Non-negative integer (usually 0-5)返回最可能的令牌及其对数概率。 Return log probabilities of the most likely tokens.no_repeat_ngram_size 不重复n元组大小0正整数 Positive integer防止重复指定长度的n元组。 Prevent repetition of n-grams of specified length. Tranining parameters\r#\r**Learning Rate:**Explanation: Controls the step size at each iteration while moving toward a minimum of the loss function. Example: 0.0001 (1e-4) **Batch Size:**Explanation: The number of training examples used in one iteration. Example: 32, 64, 128 Optimizer: Explanation: Algorithm used to update the model’s weights. Example: Adam, SGD, RMSprop **Epochs:**Explanation: The number of complete passes through the entire training dataset. Example: 10, 50, 100 **Weight Initialization:**Explanation: Method used to set the initial random weights of the neural network. Example: Xavier initialization, He initialization **Regularization:**Explanation: Techniques to prevent overfitting. Example: L2 regularization (weight decay = 0.01), Dropout (rate = 0.1) **Learning Rate Scheduler:**Explanation: Strategy to adjust the learning rate during training. Example: StepLR (step_size=30, gamma=0.1), CosineAnnealingLR **Model Architecture:**Explanation: The structure and size of the neural network. Example: Transformer with 12 layers, 768 hidden size, 12 attention heads **Sequence Length:**Explanation: The maximum length of input sequences. Example: 512, 1024, 2048 tokens **Warmup Steps:**Explanation: Number of steps to gradually increase the learning rate at the start of training. Example: 1000 steps **Gradient Clipping:**Explanation: Technique to prevent exploding gradients by limiting their magnitude. Example: max_norm=1.0 Mixed Precision Training: Explanation: Using lower precision (e.g., float16) to speed up training and reduce memory usage. Example: Enabled with float16 Distributed Training Strategy: Explanation: Method for training across multiple GPUs or nodes. Example: Data Parallel, Model Parallel **Attention Dropout:**Explanation: Dropout rate specifically for attention layers. Example: 0.1 **Activation Function:**Explanation: Non-linear function applied to neuron outputs. Example: ReLU, GELUParameterExplanationExampleLearning Rate 学习率Controls the step size at each iteration while moving toward a minimum of the loss function. 控制每次迭代时参数更新的步长。0.0001 (1e-4)Batch Size 批量大小The number of training examples used in one iteration. 每次迭代中使用的训练样本数量。32, 64, 128Optimizer 优化器Algorithm used to update the model’s weights. 用于更新模型权重的算法。Adam, SGD, RMSpropEpochs 训练轮数The number of complete passes through the entire training dataset. 完整遍历整个训练数据集的次数。10, 50, 100Weight Initialization 权重初始化Method used to set the initial random weights of the neural network. 设置神经网络初始随机权重的方法。Xavier initialization He initializationRegularization 正则化Techniques to prevent overfitting. 防止过拟合的技术。L2 regularization (weight decay = 0.01) Dropout (rate = 0.1)Learning Rate Scheduler 学习率调度器Strategy to adjust the learning rate during training. 在训练过程中调整学习率的策略。StepLR (step_size=30, gamma=0.1) CosineAnnealingLRModel Architecture 模型架构The structure and size of the neural network. 神经网络的结构和大小。Transformer with 12 layers, 768 hidden size, 12 attention headsSequence Length 序列长度The maximum length of input sequences. 输入序列的最大长度。512, 1024, 2048 tokensWarmup Steps 预热步数Number of steps to gradually increase the learning rate at the start of training. 训练开始时逐步增加学习率的步数。1000 stepsGradient Clipping 梯度裁剪Technique to prevent exploding gradients by limiting their magnitude. 通过限制梯度幅度来防止梯度爆炸的技术。max_norm=1.0Mixed Precision Training 混合精度训练Using lower precision (e.g., float16) to speed up training and reduce memory usage. 使用较低精度（如float16）来加速训练并减少内存使用。Enabled with float16Distributed Training Strategy 分布式训练策略Method for training across multiple GPUs or nodes. 跨多个GPU或节点进行训练的方法。Data Parallel, Model ParallelAttention Dropout 注意力丢弃率Dropout rate specifically for attention layers. 专门用于注意力层的丢弃率。0.1Activation Function 激活函数Non-linear function applied to neuron outputs. 应用于神经元输出的非线性函数。ReLU, GELU ","date":"14 July 2024","externalUrl":null,"permalink":"/posts/llm-parameters/","section":"Posts","summary":"\u003cp\u003e\u003cfigure\u003e\u003cimg\r\n    class=\"my-0 rounded-md\"\r\n    loading=\"lazy\"\r\n    decoding=\"async\"\r\n    fetchpriority=\"low\"\r\n    alt=\"featured\"\r\n    src=\"https://raw.githubusercontent.com/FableFatale/typora/main/img/202407141631753.jpg\"\r\n    \u003e\u003c/figure\u003e\r\n\u003c/p\u003e\n\r\n\u003ch1 class=\"relative group\"\u003eThis is an introduction to the LLM parameters.\r\n    \u003cdiv id=\"this-is-an-introduction-to-the-llm-parameters\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#this-is-an-introduction-to-the-llm-parameters\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h1\u003e\r\n\r\n\u003ch2 class=\"relative group\"\u003eInference-Parameters\r\n    \u003cdiv id=\"inference-parameters\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#inference-parameters\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h2\u003e\r\n\u003cul\u003e\n\u003cli\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTemperature(Default 1 RANGE 0-2)\u003c/strong\u003e\u003c/p\u003e","title":"LLM Parameters","type":"posts"},{"content":"\rSelf-Consistency\r#\rPrompt:\n“Please solve the following logical reasoning problem using the Self-Consistency Prompt technique. Generate at least three distinct reasoning paths, and then select the most consistent answer as the final conclusion.\nQuestion: If all birds can fly, and penguins are a type of bird, can penguins fly?\nGenerate your reasoning paths and choose the most consistent answer.”\nReasoning Path 1:\nPremise: All birds can fly. Premise: Penguins are a type of bird. Conclusion: Therefore, penguins can fly. Reasoning Path 2:\nPremise: All birds can fly. Premise: Penguins are a type of bird. Observation: However, penguins are known to be flightless. Conclusion: Therefore, penguins cannot fly. Reasoning Path 3:\nPremise: All birds can fly. Premise: Penguins are a type of bird. Observation: Penguins have adapted to a lifestyle that does not include flying. Conclusion: Therefore, penguins cannot fly. Self-Consistency Prompt Selection:\nUpon reviewing the reasoning paths, Path 1 concludes that “penguins can fly,” while Paths 2 and 3 conclude that “penguins cannot fly.” Given that Paths 2 and 3 are more consistent with the known biological facts about penguins, the most consistent answer is “penguins cannot fly.”\nFinal Conclusion:\nPenguins cannot fly.\n[!IMPORTANT]\nSelf-Consistency use in llm intern to keep logic consistency,Top-k parameters uses in restrinct output format of llm\nThink of Self-Consistency as helping the LLM think more logically like a human would debate different options, while Top-k acts like a filter, selecting the best words from a pool to make the output clearer.\n","date":"14 July 2024","externalUrl":null,"permalink":"/posts/self-consistency/","section":"Posts","summary":"\u003ch1 class=\"relative group\"\u003eSelf-Consistency\r\n    \u003cdiv id=\"self-consistency\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#self-consistency\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h1\u003e\r\n\u003cp\u003e\u003cstrong\u003ePrompt:\u003c/strong\u003e\u003c/p\u003e","title":"Self-Consistency","type":"posts"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/tags/self-consistency/","section":"Tags","summary":"","title":"Self-Consistency","type":"tags"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/flowchat/","section":"Tags","summary":"","title":"Flowchat","type":"tags"},{"content":"How can you get or update a flow chat with AI\nOpen Claude or something AI platform Input your demand or upload your flow chat ,than tell AI to get Mermaid Picture You will get a nice beautiful flow chat like this :(I use chatNext with Claude API) graph TD;\rA[User Visits Landing Page] --\u0026gt; B{Interested?}\rB --\u0026gt;|Yes| C[Explore Content]\rB --\u0026gt;|No| D[Exit Page]\rC --\u0026gt; E{Call to Action}\rE --\u0026gt;|Sign Up| F[Registration Form]\rE --\u0026gt;|Learn More| G[Additional Information]\rE --\u0026gt;|Contact Us| H[Contact Form]\rF --\u0026gt; I[Thank You Page]\rG --\u0026gt; E\rH --\u0026gt; J[Confirmation Message] ","date":"9 July 2024","externalUrl":null,"permalink":"/posts/with-ai-to-get-a-nice-flow-chat/","section":"Posts","summary":"\u003cp\u003eHow can you get or update a flow chat with AI\u003c/p\u003e","title":"With AI to get a nice flow chat","type":"posts"},{"content":"","date":"27 June 2024","externalUrl":null,"permalink":"/tags/few-shot/","section":"Tags","summary":"","title":"Few-Shot","type":"tags"},{"content":"\rFew-Shot\r#\rFew-shot learning is similar to enabling an instance to perform tasks with minimal examples.\r#\rFor instance:\nPrompt:\nPositive : \u0026#34;This is awesome!\u0026#34;\rNegative : \u0026#34;This is bad!\u0026#34;\rPositive : \u0026#34;Wow, that movie was rad!\u0026#34;\rWhat a horrible show!\u0026#34; Output:\nNegative Limitations of Few-shot\r#\rFew-shot learning may not achieve perfection in inference tasks\nFor instance:\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. A: Output:\nYes, the odd numbers in this group add up to 107, which is an even number. ","date":"27 June 2024","externalUrl":null,"permalink":"/posts/few-shot-prompt/","section":"Posts","summary":"\u003ch1 class=\"relative group\"\u003eFew-Shot\r\n    \u003cdiv id=\"few-shot\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#few-shot\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h1\u003e\r\n\r\n\u003ch3 class=\"relative group\"\u003eFew-shot learning is similar to enabling an instance to perform tasks with minimal examples.\r\n    \u003cdiv id=\"few-shot-learning-is-similar-to-enabling-an-instance-to-perform-tasks-with-minimal-examples\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#few-shot-learning-is-similar-to-enabling-an-instance-to-perform-tasks-with-minimal-examples\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003eFor instance:\u003c/p\u003e","title":"Few-Shot Prompt","type":"posts"},{"content":"","date":"26 June 2024","externalUrl":null,"permalink":"/tags/zero-shot/","section":"Tags","summary":"","title":"Zero-Shot","type":"tags"},{"content":"\rZero-Shot\r#\rThis is my second blog\r#\rMy bad chinese-english:\nFrom my view , zero-shot more like a Q\u0026amp;A ,a question a answer.\nwhen I input ,llm begin work and then it will tell me about mine input\nThis is a instance :\nLLM enhance result:\nFrom my perspective, zero-shot resembles a Q\u0026amp;A format, where each question is followed by an answer.\nWhen I input a query, the language model begins working and then provides feedback on my input.\nHere is an example:\n","date":"26 June 2024","externalUrl":null,"permalink":"/posts/zero-shot-prompt/","section":"Posts","summary":"\u003ch1 class=\"relative group\"\u003eZero-Shot\r\n    \u003cdiv id=\"zero-shot\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#zero-shot\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h1\u003e\r\n\r\n\u003ch3 class=\"relative group\"\u003eThis is my second blog\r\n    \u003cdiv id=\"this-is-my-second-blog\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#this-is-my-second-blog\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h3\u003e\r\n\u003cp\u003e\u003cstrong\u003eMy bad chinese-english:\u003c/strong\u003e\u003c/p\u003e","title":"Zero-Shot Prompt","type":"posts"},{"content":"\rA sub-title\r#\rThis is my first post on Hugo,thank you for your reading\nI am practice english\n","date":"23 June 2024","externalUrl":null,"permalink":"/posts/my-first-post/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003eA sub-title\r\n    \u003cdiv id=\"a-sub-title\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#a-sub-title\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003eThis is my first post on Hugo,thank you for your reading\u003c/p\u003e","title":"My first post","type":"posts"},{"content":"","date":"23 June 2024","externalUrl":null,"permalink":"/tags/space/","section":"Tags","summary":"","title":"Space","type":"tags"},{"content":"\r标题\r#\rThis is my first post on Hugo,thank you for your reading\n我正在学习英语\n","date":"2024年6月23日","externalUrl":null,"permalink":"/zh-cn/posts/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","section":"Posts","summary":"\u003ch2 class=\"relative group\"\u003e标题\r\n    \u003cdiv id=\"标题\" class=\"anchor\"\u003e\u003c/div\u003e\r\n    \r\n    \u003cspan\r\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\"\u003e\r\n        \u003ca class=\"text-primary-300 dark:text-neutral-700 !no-underline\" href=\"#%e6%a0%87%e9%a2%98\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\r\n    \u003c/span\u003e\r\n    \r\n\u003c/h2\u003e\r\n\u003cp\u003eThis is my first post on Hugo,thank you for your reading\u003c/p\u003e","title":"我的第一篇博客","type":"posts"},{"content":"","date":"2024年6月23日","externalUrl":null,"permalink":"/zh-cn/tags/%E4%B8%AD%E6%96%87/","section":"Tags","summary":"","title":"中文","type":"tags"},{"content":"欢迎来到我的博客！\n这是一个使用 Hugo 和 Blowfish 主题构建的个人博客。\n关于我\r#\r软件开发者 \u0026amp; AI 爱好者\n联系方式\r#\rGitHub: FableFatale Twitter: @wangye1644294 ","date":"2024年1月1日","externalUrl":null,"permalink":"/zh-cn/about/","section":"Rafael Yu's Blog","summary":"\u003cp\u003e欢迎来到我的博客！\u003c/p\u003e","title":"关于我","type":"page"},{"content":"","externalUrl":null,"permalink":"/zh-cn/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/zh-cn/series/","section":"Series","summary":"","title":"Series","type":"series"}]