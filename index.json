


[{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/tags/gpt/","section":"Tags","summary":"","title":"Gpt","type":"tags"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM","type":"tags"},{"content":"\rThis is an introduction to the LLM parameters. #\rInference-Parameters #\rTemperature(Default 1 RANGE 0-2)\nhigh temperature will enhance llm\u0026rsquo;s personal, less temperature will keep llm\u0026rsquo;s level\nfor instance: while temperature =1.0, llm\u0026rsquo;s behavior will more brave , 0.1 will be stable\nA higher temperature setting can enhance the LLM\u0026rsquo;s creativity, while a lower temperature will maintain its consistency.\nFor example: When the temperature is set to 1.0, the LLM\u0026rsquo;s responses tend to be more adventurous; at 0.1, they are more conservative.\nMax Generation Length(Default: Model-dependent,Range 1 to model\u0026rsquo;s maximum)\nSet Max Generation Length can control llm\u0026rsquo;s input context\u0026rsquo;s length\nSetting the Maximum Generation Length can control the length of the input context for the LLM.\u0026quot;\nTop-K(Default 0 Range 1 to vocabulary,Common Range 40,50)\nSampling is restricted to the top k highest-probability words. For instance, with k set to 50, the selection is made from among the 50 most likely words.\nTop-p(Default 1 Range0-1 ,Common 0.9,0.95)\nSampling is conducted from the pool of words until their cumulative probability reaches p. For example, with p set to 0.9, the selection is made from among the words whose combined probabilities account for 90% of the total.\nRepetion Penalty(Default 1 Range 1.0- 2.0)\nTo mitigate the likelihood of generating repetitive words, a higher repetition penalty Range can be applied, which effectively reduces the occurrence of redundant content in the text.\nBeam Searth(Default 1,Range 1 to 10)\nBeam Search is a heuristic search algorithm that maintains multiple candidate sequences during generation, ultimately selecting the most optimal text output. Key parameters for this algorithm include the Beam Width, which determines the number of candidates retained at each step.\nLength Penalty\nLength Penalty is a setting used in text generation to help the model create text that\u0026rsquo;s not too short or too long. It works by adjusting the score of a generated text based on its length. Example: Imagine you\u0026rsquo;re asking a computer to write a summary of a book. Without Length Penalty, the summary might be too short and miss important points, or it might be too long and include unnecessary details. By using Length Penalty, you can tell the computer to aim for a summary that\u0026rsquo;s just the right length, so it includes all the key information without extra fluff.\nfrequency_penalty(Default 0 Range -2.0- +2.0)\nDescription: Penalizes words based on their frequency in the generated text, reducing repetition. Example: When set to 1.0, common words are used less often, resulting in more diverse text.\npresence_penalty(Default 0 Range -2.0- +2.0)\nDescription: Penalizes words that have already appeared in the text, encouraging the use of new words. Example: When set to 1.5, the model tends to use words that haven\u0026rsquo;t appeared yet, increasing text diversity.\nstop(Range:String or String list)\nDescription: Specifies one or more tokens at which to stop generation. Example: When set to [\u0026quot;.\u0026quot;, \u0026ldquo;!\u0026rdquo;, \u0026ldquo;?\u0026rdquo;], the model will stop generating after producing these punctuation marks.\nn (Default 1)\nDescription: Specifies how many completions to generate. Example: When set to 3, the model will generate 3 different responses.\nbest_of\nDescription: Generates multiple candidate results and returns the best n. Example: With n=2 and best_of=5, the model generates 5 candidate results and returns the 2 best ones.\nlogprobs\nDescription: Returns the most likely tokens and their log probabilities. Example: When set to 3, each generated token will be accompanied by the 3 most likely alternative options and their probabilities.\nno_repeat_ngram_size\nDescription: Prevents repetition of specified length word groups. Example: When set to 3, the model avoids generating any consecutive repetition of three-word combinations.\n[!CAUTION]\nThese parameters can be used together, but with some limitations:\nSome parameters work against each other, like high temperature with strict top-k/top-p. Beam search often overrides other sampling methods. Using many complex parameters at once can slow down generation. Not all models support every parameter. Some parameters have specific Range ranges (e.g., temperature is usually 0-2). Different tasks may need different parameter combinations. Stop conditions usually take priority over other parameters. Best practice:\nStart with defaults Adjust one parameter at a time Test thoroughly Keep notes on what works best 参数名称\nParameter Name 默认值\nDefault Value 取值范围\nRange 描述\nDescription Temperature\n温度 1 0-2 控制输出的随机性和创造性。较高值增加创造性，较低值增加一致性。\nControls randomness and creativity of output. Higher values increase creativity, lower values increase consistency. Max Generation Length\n最大生成长度 模型相关\nModel-dependent 1 到模型最大值\n1 to model\u0026rsquo;s maximum 控制LLM输入上下文的长度。\nControls the length of the input context for the LLM. Top-K\n前K个 0 1 到词汇表大小\n1 to vocabulary size 限制采样到概率最高的K个词。常用范围40-50。\nRestricts sampling to top K highest-probability words. Common range 40-50. Top-p\n前p个 1 0-1 从累积概率达到p的词池中采样。常用值0.9, 0.95。\nSamples from words until cumulative probability reaches p. Common values 0.9, 0.95. Repetition Penalty\n重复惩罚 1 1.0-2.0 降低重复词的生成概率，减少冗余内容。\nReduces likelihood of generating repetitive words, decreasing redundant content. Beam Search\n束搜索 1 1-10 保持多个候选序列，选择最优输出。关键参数包括Beam Width。\nMaintains multiple candidate sequences, selects optimal output. Key parameter includes Beam Width. Length Penalty\n长度惩罚 1.0 通常0.0到2.0\nUsually 0.0 to 2.0 根据长度调整生成文本的分数，控制输出长度。\nAdjusts the score of generated text based on its length, controlling output length. frequency_penalty\n频率惩罚 0 -2.0 到 2.0\n-2.0 to 2.0 根据词频对令牌进行惩罚。\nPenalizes tokens based on their frequency. presence_penalty\n存在惩罚 0 -2.0 到 2.0\n-2.0 to 2.0 根据是否出现过对令牌进行惩罚。\nPenalizes tokens based on their presence. stop\n停止 无\nNone 字符串或字符串列表\nString or String list 指定停止生成的标记。\nSpecifies tokens at which to stop generation. n\n数量 1 正整数\nPositive integer 生成多少个完成结果。\nNumber of completions to generate. best_of\n最佳数量 1 正整数，大于等于n\nPositive integer, ≥ n 生成多个候选结果并返回最佳的n个。\nGenerate multiple candidates and return the best n. logprobs\n对数概率 null 非负整数（通常0-5）\nNon-negative integer (usually 0-5) 返回最可能的令牌及其对数概率。\nReturn log probabilities of the most likely tokens. no_repeat_ngram_size\n不重复n元组大小 0 正整数\nPositive integer 防止重复指定长度的n元组。\nPrevent repetition of n-grams of specified length. Tranining parameters #\rLearning Rate: Explanation: Controls the step size at each iteration while moving toward a minimum of the loss function. Example: 0.0001 (1e-4) Batch Size: Explanation: The number of training examples used in one iteration. Example: 32, 64, 128 Optimizer: Explanation: Algorithm used to update the model\u0026rsquo;s weights. Example: Adam, SGD, RMSprop Epochs: Explanation: The number of complete passes through the entire training dataset. Example: 10, 50, 100 Weight Initialization: Explanation: Method used to set the initial random weights of the neural network. Example: Xavier initialization, He initialization Regularization: Explanation: Techniques to prevent overfitting. Example: L2 regularization (weight decay = 0.01), Dropout (rate = 0.1) Learning Rate Scheduler: Explanation: Strategy to adjust the learning rate during training. Example: StepLR (step_size=30, gamma=0.1), CosineAnnealingLR Model Architecture: Explanation: The structure and size of the neural network. Example: Transformer with 12 layers, 768 hidden size, 12 attention heads Sequence Length: Explanation: The maximum length of input sequences. Example: 512, 1024, 2048 tokens Warmup Steps: Explanation: Number of steps to gradually increase the learning rate at the start of training. Example: 1000 steps Gradient Clipping: Explanation: Technique to prevent exploding gradients by limiting their magnitude. Example: max_norm=1.0 Mixed Precision Training: Explanation: Using lower precision (e.g., float16) to speed up training and reduce memory usage. Example: Enabled with float16 Distributed Training Strategy: Explanation: Method for training across multiple GPUs or nodes. Example: Data Parallel, Model Parallel Attention Dropout: Explanation: Dropout rate specifically for attention layers. Example: 0.1 Activation Function: Explanation: Non-linear function applied to neuron outputs. Example: ReLU, GELU Parameter Explanation Example Learning Rate\n学习率 Controls the step size at each iteration while moving toward a minimum of the loss function.\n控制每次迭代时参数更新的步长。 0.0001 (1e-4) Batch Size\n批量大小 The number of training examples used in one iteration.\n每次迭代中使用的训练样本数量。 32, 64, 128 Optimizer\n优化器 Algorithm used to update the model\u0026rsquo;s weights.\n用于更新模型权重的算法。 Adam, SGD, RMSprop Epochs\n训练轮数 The number of complete passes through the entire training dataset.\n完整遍历整个训练数据集的次数。 10, 50, 100 Weight Initialization\n权重初始化 Method used to set the initial random weights of the neural network.\n设置神经网络初始随机权重的方法。 Xavier initialization\nHe initialization Regularization\n正则化 Techniques to prevent overfitting.\n防止过拟合的技术。 L2 regularization (weight decay = 0.01)\nDropout (rate = 0.1) Learning Rate Scheduler\n学习率调度器 Strategy to adjust the learning rate during training.\n在训练过程中调整学习率的策略。 StepLR (step_size=30, gamma=0.1)\nCosineAnnealingLR Model Architecture\n模型架构 The structure and size of the neural network.\n神经网络的结构和大小。 Transformer with 12 layers, 768 hidden size, 12 attention heads Sequence Length\n序列长度 The maximum length of input sequences.\n输入序列的最大长度。 512, 1024, 2048 tokens Warmup Steps\n预热步数 Number of steps to gradually increase the learning rate at the start of training.\n训练开始时逐步增加学习率的步数。 1000 steps Gradient Clipping\n梯度裁剪 Technique to prevent exploding gradients by limiting their magnitude.\n通过限制梯度幅度来防止梯度爆炸的技术。 max_norm=1.0 Mixed Precision Training\n混合精度训练 Using lower precision (e.g., float16) to speed up training and reduce memory usage.\n使用较低精度（如float16）来加速训练并减少内存使用。 Enabled with float16 Distributed Training Strategy\n分布式训练策略 Method for training across multiple GPUs or nodes.\n跨多个GPU或节点进行训练的方法。 Data Parallel, Model Parallel Attention Dropout\n注意力丢弃率 Dropout rate specifically for attention layers.\n专门用于注意力层的丢弃率。 0.1 Activation Function\n激活函数 Non-linear function applied to neuron outputs.\n应用于神经元输出的非线性函数。 ReLU, GELU ","date":"14 July 2024","externalUrl":null,"permalink":"/posts/llm_parameters/llm-parameters/","section":"Posts","summary":"LLM\u0026rsquo;s parameters study","title":"LLM Parameters","type":"posts"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/tags/prompt/","section":"Tags","summary":"","title":"Prompt","type":"tags"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/","section":"Rafael Blue's Blog","summary":"","title":"Rafael Blue's Blog","type":"page"},{"content":"","date":"14 July 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"9 July 2024","externalUrl":null,"permalink":"/tags/flowchatskillscsai/","section":"Tags","summary":"","title":"Flowchat，skills,cs,AI","type":"tags"},{"content":"How can you get or update a flow chat with AI\nOpen Claude or something AI platform Input your demand or upload your flow chat ,than tell AI to get Mermaid Picture You will get a nice beautiful flow chat like this :(I use chatNext with Claude API) graph TD\rA[User Visits Landing Page] --\u0026gt; B{Interested?}\rB --\u0026gt;|Yes| C[Explore Content]\rB --\u0026gt;|No| D[Exit Page]\rC --\u0026gt; E{Call to Action}\rE --\u0026gt;|Sign Up| F[Registration Form]\rE --\u0026gt;|Learn More| G[Additional Information]\rE --\u0026gt;|Contact Us| H[Contact Form]\rF --\u0026gt; I[Thank You Page]\rG --\u0026gt; E\rH --\u0026gt; J[Confirmation Message] ​\n","date":"9 July 2024","externalUrl":null,"permalink":"/posts/how--can---you---get-or-update--a-flow-chat---with-ai/","section":"Posts","summary":"How can you get or update a flow chat with AI","title":"With AI to get a nice flow chat","type":"posts"},{"content":"","date":"26 June 2024","externalUrl":null,"permalink":"/tags/promptskillscszero-shot/","section":"Tags","summary":"","title":"Prompt，skills,cs,zero-Shot","type":"tags"},{"content":"\rZero-Shot #\rThis is my second blog #\rMy bad chinese-english:\nFrom my view , zero-shot more like a Q\u0026amp;A ,a question a answer.\nwhen I input ,llm begin work and then it will tell me about mine input\nThis is a instance :\nLLM enhance result:\nFrom my perspective, zero-shot resembles a Q\u0026amp;A format, where each question is followed by an answer.\nWhen I input a query, the language model begins working and then provides feedback on my input.\nHere is an example:\n","date":"26 June 2024","externalUrl":null,"permalink":"/posts/prompt-zeroshot/","section":"Posts","summary":"This is my first post on my site on 2024/6/26","title":"Zero-Shot Prompt","type":"posts"},{"content":"\rA sub-title #\rThis is my first post on Hugo,thank you for your reading\nI am practice english\n","date":"23 June 2024","externalUrl":null,"permalink":"/posts/first/","section":"Posts","summary":"This is my first post on my site on 2024/6/23","title":"My first post","type":"posts"},{"content":"","date":"23 June 2024","externalUrl":null,"permalink":"/tags/space/","section":"Tags","summary":"","title":"Space","type":"tags"},{"content":"","date":"23 June 2024","externalUrl":null,"permalink":"/zh-cn/tags/%E4%B8%AD%E6%96%87/","section":"Tags","summary":"","title":"中文","type":"tags"},{"content":"\r标题 #\rThis is my first post on Hugo,thank you for your reading\n我正在学习英语\n","date":"23 June 2024","externalUrl":null,"permalink":"/zh-cn/posts/%E7%AC%AC%E4%B8%80%E7%AF%87/","section":"Posts","summary":"This is my first post on my site on 2024/6/23","title":"我的第一篇博客","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]